# Atlassian Multi-Tenancy at 3 Million Tenants: The TiDB Migration

> **Source**: [Multi-Tenancy at Atlassian Scale](https://www.youtube.com/watch?v=asnCJI39MpE)

> [!IMPORTANT]
> **The Scale**: Atlassian manages **3 million tenants** for Jira, Confluence, and Trello. With **800+ tables per product** and **700+ third-party plugins** each adding their own schema, the challenge isn't query volumeâ€”it's **metadata explosion**.

---

## ðŸ“Š Why Traditional Models Fail at This Scale

![Multi-Tenancy Models at 3 Million Tenants](assets/multitenancy-models-3m.png)

---

## ðŸ¢ The Enterprise Requirements

Atlassian operates in high-compliance environments (FedRAMP) with strict requirements:

| Requirement | Description |
| :--- | :--- |
| **Encryption at Rest (BYOK)** | Customers bring their own keys. Per-tenant encryption required. |
| **Data Residency** | Pin or migrate tenant data to specific regions. |
| **Point-in-Time Recovery** | Restore a specific tenant's data to a specific moment. |

> [!CAUTION]
> These requirements are **impossible** with a simple shared `tenant_id` column. You can't encrypt, migrate, or restore one tenant's data efficiently when it's mixed with millions of others.

---

## ðŸ”´ Multi-Tenancy Models: Why They All Failed

### Model 1: Shared (Discriminator Column)
*   Single database, `tenant_id` column.
*   âœ… Simple to operate.
*   âŒ Can't do per-tenant encryption.
*   âŒ Can't migrate data for residency.
*   âŒ Can't restore single tenant.

### Model 2: Silo (Database-per-Tenant)
*   Every tenant gets their own database + schema.
*   âœ… Perfect isolation for operations.
*   âŒ **Metadata explosion**: `3M tenants Ã— 800 tables = 2.4 billion objects`.
*   âŒ Most databases have a **1 million object limit**.
*   âŒ DDL operations take forever.

### Model 3: Hybrid (99% Shared, 1% Silo)
*   Enterprise tenants get dedicated databases.
*   âŒ At Atlassian scale: `1% Ã— 3M = 30,000 databases`.
*   âŒ With 700 plugins: `30K Ã— 700 = 21 million databases`.
*   âŒ Back to silo problems.

---

## ðŸ”„ Legacy Platform: Sharded Postgres

![Postgres Sharding vs TiDB](assets/postgres-vs-tidb.png)

### How It Worked
*   ~1,000 Postgres clusters (shards).
*   1,000 tenants per shard (2,000:1 bin packing ratio).

### The Pain Points

| Component | Problem |
| :--- | :--- |
| **Connection Pooler** | Thread-per-connection consumes massive memory. |
| **Tenant Placement Service** | Complex logic to assign tenants to shards. |
| **Shard Rebalancer** | Constantly avoiding "hot spots". |
| **Operations** | Difficult upgrades, complex monitoring. |

---

## âœ… The TiDB Solution

### Why TiDB?
*   **Goal**: Increase bin packing from 2,000:1 to **200,000:1**.
*   **Result**: Consolidate ~1,000 Postgres clusters into **16 TiDB clusters**.

### Technical Solutions

![TiDB Metadata Scaling](assets/tidb-metadata-scaling.png)

| Solution | How It Works | Result |
| :--- | :--- | :--- |
| **Connection Management** | Multi-master topology + lightweight sessions | **500K concurrent connections** per cluster |
| **Lazy Metadata Loading** | Only active tenants (~1%) loaded into memory | **Node startup: 20 min â†’ 2 min** |
| **DDL Throughput** | Optimized DDL pipeline | **7,000 DDLs/min** (was 1,000) |
| **Backup & Restore** | BR tooling optimized for massive metadata | Per-tenant PITR possible |

### Configuration Tuning
*   Tuned **region sizes** for high metadata volume.
*   Disabled **automatic table splitting** to manage object count.

---

## ðŸ“Š Results & Current Status

| Metric | Before (Postgres) | After (TiDB) |
| :--- | :--- | :--- |
| **Clusters** | ~1,000 | 16 |
| **Bin Packing Ratio** | 2,000:1 | 200,000:1 |
| **Improvement** | â€” | **50x** |
| **Upgrades** | Downtime required | No-downtime possible |

### Migration Roadmap
*   âœ… **Phase 1**: Smaller products (Atlas, Loom).
*   ðŸ”„ **Phase 2**: Major products (Jira, Confluence).

---

## âœ… Principal Architect Checklist

1.  **Calculate Your Metadata Volume**: At 3M tenants Ã— 800 tables, you have 2.4B objects. Most databases can't handle this.
2.  **Evaluate Lazy Loading**: If only 1% of tenants are active, you don't need 100% of metadata in memory.
3.  **Measure DDL Throughput**: Adding a column to 3M schemas means 3M DDL operations. What's your DDL/minute rate?
4.  **Consider Bin Packing Ratio**: If you're at 2,000:1, you need 1,500 clusters for 3M tenants. Is that manageable?
5.  **Test Connection Scaling**: Thread-per-connection doesn't scale. Look for lightweight session architectures.
6.  **Plan for Enterprise Requirements**: BYOK, data residency, and PITR require tenant isolation at the storage level.

---

## ðŸ“– Analogy: The Hotel with Millions of Rooms

> [!TIP]
> Scaling 3 million tenants is like **managing a massive hotel**:
>
> *   **Shared Model**: One giant communal room where everyone's luggage is mixed but tagged. Impossible to give one person a private key to just their suitcase.
>
> *   **Silo Model**: Every guest gets their own room with 800 drawers. The front desk can't remember where all the rooms are.
>
> *   **TiDB**: A modern skyscraper with a **digital directory** (lazy metadata) and **automatic elevators** (distributed compute). The front desk only thinks about guests currently in the lobbyâ€”allowing infinite scale without overwhelm.

---

## ðŸ”— Related Documents
*   [Multi-Tenancy with Spring](multi-tenancy-spring.md) â€” Application-level implementation
*   [Cortex: Multi-Tenant Prometheus](cortex-prometheus-scale.md) â€” Observability at scale